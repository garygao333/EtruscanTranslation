{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AI Etruscan Project\n",
        "\n",
        "Abstract—Etruscan is an ancient language spoken in Italy from the 7th century BC to the 12th century AD. The language is both lost and isolated, making its resources extremely scarce. In this project, we will propose a state-of-the art design that integrates a transformer model architecture with a BPE with dropout tokenizer, adaptive transformer layers, and a back-translation mechanism.\n"
      ],
      "metadata": {
        "id": "ySA40lQnDlS2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing\n",
        "\n",
        "For the following code lines, we will use the pip, Python's package installer, to install packages. Also, anytime you want to run a terminal command, put the ! mark in the front!"
      ],
      "metadata": {
        "id": "mv3bW6DJY7KP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyXQYy9g6KRF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fd26020-bfe7-4bc7-eee0-dc5c3b31df57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchdata in /usr/local/lib/python3.10/dist-packages (0.7.1)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.31.0)\n",
            "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2->torchdata) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2->torchdata) (1.3.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.16.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.15.1)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2024.2.2)\n",
            "Requirement already satisfied: portalocker==2.8.2 in /usr/local/lib/python3.10/dist-packages (2.8.2)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.1)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.25.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.2)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (15.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U torchdata  # Installs/updates the 'torchdata' library, which provides data processing utilities and datasets for PyTorch.\n",
        "!pip install -U spacy      # Installs/updates the 'spacy' library, a popular NLP library for advanced natural language processing tasks.\n",
        "!pip install tokenizers # Installs the necessary tokenizers from Huggingface\n",
        "!pip install 'portalocker==2.8.2'  # Installs/updates the 'portalocker' library used for file locking - a mechanism that allows you to restrict access to a file by allowing only one process to read or write the file at once.\n",
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will also be installing from SpaCy's library, which uses the command 'el_core_news_sm.'"
      ],
      "metadata": {
        "id": "rQ6N1iVVZzCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download el_core_news_sm"
      ],
      "metadata": {
        "id": "j-3vgaVFaj1V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55646ca7-0379-428e-dfbe-fc441375fcc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting el-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/el_core_news_sm-3.7.0/el_core_news_sm-3.7.0-py3-none-any.whl (12.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from el-core-news-sm==3.7.0) (3.7.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (2.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (2.16.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (2.1.5)\n",
            "Installing collected packages: el-core-news-sm\n",
            "Successfully installed el-core-news-sm-3.7.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('el_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will install a small English, German, and Greek model that includes capabilities for tokenization, lemmatization, POS tagging, named entity recognition, and more! It is opitmized to process web text."
      ],
      "metadata": {
        "id": "46X2IFX-axV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm # This is for English\n",
        "!python -m spacy download de_core_news_sm # This is for German\n",
        "!python -m spacy download el_core_news_sm # This is for Greek"
      ],
      "metadata": {
        "id": "jnyiR14FbHwz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4186ebc6-332b-4bc0-f248-70ce22ea3637"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "Collecting de-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.7.0/de_core_news_sm-3.7.0-py3-none-any.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from de-core-news-sm==3.7.0) (3.7.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.16.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.1.5)\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-3.7.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n",
            "Collecting el-core-news-sm==3.7.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/el_core_news_sm-3.7.0/el_core_news_sm-3.7.0-py3-none-any.whl (12.6 MB)\n",
            "Requirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from el-core-news-sm==3.7.0) (3.7.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (2.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (2.16.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->el-core-news-sm==3.7.0) (2.1.5)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('el_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up the Transformer\n",
        "\n",
        "We will now begin to set up the transformer, defining its parameters and importing the necessary items. We will first use the '%matplotlib inline' magic command, which tells the Jupyter Notebook to dispaly matplotlib(plotting library) plots directly on the notebook rather than opening a new window.\n"
      ],
      "metadata": {
        "id": "Rkp-aVD3bseh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "#Displaying plots directly onto the notebook and renders them as static images."
      ],
      "metadata": {
        "id": "s7GLjwhUrwGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will define the traning and validation data paths and langauge types using PyTorch's 'torchtext' library. Defining the path involves specifying the file and location where the training and validation data sets are stored."
      ],
      "metadata": {
        "id": "_cTV5aI8sqhr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initializing the BPE Tokenizer"
      ],
      "metadata": {
        "id": "sAtSoMewA4Aq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KL1_0PHqWtPV",
        "outputId": "3d42300b-93fd-4e0e-d345-3817731ae299"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -c \"import evaluate; print(evaluate.load('exact_match').compute(references=['hello'], predictions=['hello']))\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWAcem3MIk3F",
        "outputId": "72c5f948-e417-41c5-827c-18cdcd766e5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-02-14 04:08:47.806255: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-02-14 04:08:47.806311: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-02-14 04:08:47.807697: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-02-14 04:08:49.070019: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Downloading builder script: 100% 5.67k/5.67k [00:00<00:00, 12.7MB/s]\n",
            "{'exact_match': 1.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "import torch\n",
        "from torchtext.datasets import Multi30k\n",
        "bleu = evaluate.load(\"bleu\") #Importing the BLEU\n",
        "\n",
        "Assuming SRC_LANGUAGE and TGT_LANGUAGE are defined\n",
        "SRC_LANGUAGE = 'de'  # Example source language\n",
        "TGT_LANGUAGE = 'en'  # Example target language\n",
        "\n",
        "# Initialize an empty BPE tokenizer\n",
        "# Enable dropout - expeirment with the dropout rate\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "# Trainer for the BPE tokenizer\n",
        "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "\n",
        "def get_texts(language_pair):\n",
        "    for ln in language_pair:\n",
        "        train_iter = Multi30k(split='train', language_pair=language_pair)\n",
        "        for data_sample in train_iter:\n",
        "            yield data_sample[0 if ln == SRC_LANGUAGE else 1]\n",
        "\n",
        "# Train the tokenizer\n",
        "tokenizer.train_from_iterator(get_texts((SRC_LANGUAGE, TGT_LANGUAGE)), trainer=trainer)\n",
        "\n",
        "# Example function to encode text using the trained BPE tokenizer\n",
        "def encode_text(text):\n",
        "    return tokenizer.encode(text).tokens\n",
        "\n",
        "# Save the tokenizer\n",
        "tokenizer.save(\"/content/drive/MyDrive/Etruscan_Project/bpe_tokenizer.json\")"
      ],
      "metadata": {
        "id": "hXh89H3VBC-H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "e6a4b280-309a-4275-e93f-b7709d386700"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-8-c0e0f13fa225>, line 9)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-c0e0f13fa225>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    Assuming SRC_LANGUAGE and TGT_LANGUAGE are defined\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the existing BPE tokenizer."
      ],
      "metadata": {
        "id": "agQ686JtCNNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "# example_text = \"INAS, INNI, INV; MAROS.\"\n",
        "# print(encode_text(example_text))"
      ],
      "metadata": {
        "id": "gA3_etLsCQjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#First, we will import the necessary utilities for handling text data. These utilities include:\n",
        "from torchtext.data.utils import get_tokenizer #A function to get a tokenizer based on the specified language (or a custom tokenization function)\n",
        "from torchtext.vocab import build_vocab_from_iterator #This function builds a vocabulary object that maps tokens to indicies. A vocabulary is a collection of unqiue tokens in a dataset.\n",
        "# A vocabulary object is a data structure that organizes and maps tokens to unique numerical indicies(an integer), which allows any text to be represented as a sequence of integers - allowing the computer to read it.\n",
        "from torchtext.datasets import multi30k, Multi30k #References the Multi30K dataset\n",
        "#from datasets import load_dataset #Preparing to load the huggingface dataset\n",
        "\n",
        "from typing import Iterable, List #Importing some typing utilities for better code readibility and type checking.\n",
        "\n",
        "#dataset = load_dataset(\"latin_english_translation\", split='train') #We are now loading the huggingface dataset\n",
        "\n",
        "#This code defines the training and validation data paths (for the Multi30k Dataset).\n",
        "#These URLs point to a GitHub repository that hosts the dataset files. This ensures that the datasets can be accessed and used for training and validation reasons.\n",
        "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\" # This is for the training dataset!\n",
        "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\" #This is for the validation dataset!\n",
        "\n",
        "#The code below will define the source and target languages using the ISO language codes. In our case, we are translating from German, which is our source, to English, which is our target.\n",
        "SRC_LANGUAGE = 'de' #de is the ISO language code for German\n",
        "TGT_LANGUAGE = 'en' #en is the ISO language code for English\n",
        "\n",
        "#Now, we will initialize dictionaries(unordered key-value pair data structure) to hold specific transformer functions and vocabulary transformations for each language.\n",
        "#These specific tokenization fuctions and vocabularies will map tokens to numerical indicies, essential for processing textual data in ML models.\n",
        "token_transform = {} #Initializes the 'token_transform' dictionary to hold tokenization functions.\n",
        "vocab_transform = {} #Initializes the 'vocab_transform' dictionary to hold vocabulary transformation functions."
      ],
      "metadata": {
        "id": "ujlvCChEv3iO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The '!pwd' (print working directory) command is a shell command that outputs the absolute path of the current directory you are in."
      ],
      "metadata": {
        "id": "gGy4R6nR0UyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "pslQjIN702S6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will then connect Google Colab to Google Drive so that the notebook can access files stored in Google Drive."
      ],
      "metadata": {
        "id": "otby-YZH03Qx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive #The 'drive' module provides functions to interact with Google Drive.\n",
        "drive.mount('/content/drive')# This command will prompt the user to authorize access to their Google Drive."
      ],
      "metadata": {
        "id": "X3Y2XSGR1EIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next two commands will upgrade the 'torchdata' and 'spacy' library to the latest version."
      ],
      "metadata": {
        "id": "00i7y-EC1v2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U torchdata #Upgrading(-U) the 'torchdata' library to the latest version.\n",
        "!pip install -U spacy #Upgrading the 'spacy' library to the latest version."
      ],
      "metadata": {
        "id": "LOYmjTkY17lS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Do I need 'vocab_transform' here without anything else??**"
      ],
      "metadata": {
        "id": "x_1MUmZ82a93"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bn5C1jbVTQ0d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets now import the necessary items for the BPE Dropout Tokenizer! **Make sure to figure out if we have a pre-trained BPE tokenizer that we can load for Greek.**"
      ],
      "metadata": {
        "id": "BtNgs6ahSkx0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "\n",
        "# For demonstration, let's assume you have a pre-trained tokenizer file\n",
        "tokenizer_path = 'bpe_tokenizer.json'\n",
        "\n",
        "# Load the tokenizer\n",
        "bpe_tokenizer = Tokenizer.from_file('/content/drive/MyDrive/Etruscan_Project/bpe_tokenizer.json')\n",
        "\n",
        "# Enable dropout - expeirment with the dropout rate\n",
        "#bpe_tokenizer.enable_dropout(0.1) # 0.1 is the dropout rate\n"
      ],
      "metadata": {
        "id": "tZvS5TDkSqDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will set up and define the necessary components for handling and transforming text data from German to English using the 'torchtext' and 'spacy' libraries."
      ],
      "metadata": {
        "id": "Vq2kwQMS2hfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Here, we are defining 'token_transform' as a dictionary that can hold tokenization functions for the source (SRC_LANGUAGE) and target (TGT_LANGUAGE) languages.\n",
        "#The keys for 'token_transform' are the language codes, and the value are the functions that tokenizes the texts for their respective languages.\n",
        "#The get_tokenizer function is called with the SpaCy model for each language, providing the appropriate functions to tokenize the text.\n",
        "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')  # German tokenizer\n",
        "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')  # English tokenizer\n",
        "\n",
        "#This is a helper function that iterates over a dataset, tokenizes the text samples according to a specified language, and yields a list of tokens.\n",
        "#'data_iter' is a paramater that is an iterable collection of data samples, where each sample consist of text data. The iterable collection could be a list or anything else.\n",
        "#'language' is a paramter that specifies the language of the text that is going to be tokenized.\n",
        "# def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
        "#     language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1} #This dictionary maps the language identifiers to indicies, which are then used to select the correct part of the data sample corresponding to the specified language.\n",
        "\n",
        "#     for data_sample in data_iter:\n",
        "#         # The function loops through each 'data_sample' in 'data_iter,' tokenizing based on the specific 'language.' This involves breaking text into words, punctuation, etc., which are the basic units for processing.\n",
        "#         yield token_transform[language](data_sample[language_index[language]]) #This line is where the tokenization happens.\n",
        "#         #The function retrieves the appropriate tokenization function based on the language parameter, applies it to the text selected by language_index[language], and yields the list of tokens produced.\n",
        "\n",
        "#The SpaCy tokenizer is replaced with the BPE_Dropout Tokenizer\n",
        "#This is a helper function that iterates over a dataset, tokenizes the text samples according to a specified language, and yields a list of tokens.\n",
        "#'data_iter' is a paramater that is an iterable collection of data samples, where each sample consist of text data. The iterable collection could be a list or anything else.\n",
        "#'language' is a paramter that specifies the language of the text that is going to be tokenized.\n",
        "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
        "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1} #This dictionary maps the language identifiers to indicies, which are then used to select the correct part of the data sample corresponding to the specified language.\n",
        "    for data_sample in data_iter:\n",
        "       # The function loops through each 'data_sample' in 'data_iter,' tokenizing based on the specific 'language.' This involves breaking text into words, punctuation, etc., which are the basic units for processing.\n",
        "        if language == SRC_LANGUAGE:\n",
        "            tokens = bpe_tokenizer.encode(data_sample[language_index[language]]).tokens\n",
        "        else:\n",
        "            # Assuming you have a separate tokenizer for the target language, or use the same with different settings\n",
        "            tokens = bpe_tokenizer.encode(data_sample[language_index[language]]).tokens\n",
        "        yield tokens\n",
        "\n",
        "#The break here is probably a mistake\n",
        "\n",
        "#Here, we are defining special symbols that are used in NLP tasks, such as <unk> for unknown tokens, <pad> for padding, <bos> for the beginning of a sentence, and <eos> for the end of a sentence.\n",
        "#These tokens are necessary for handling various scenarios in text processing and model training - such as padding sequences to a uniform length in collation.\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "# This for loop will iterate over both languages (German and English), building vocabularies for each.\n",
        "#The vocabularies map unique tokens to indices based on the Multi30k Model. The vocabularies include the special symbols defined above, ensuring they are part of the vocabulary and properly indexed.\n",
        "#This loop will iterate over the source and target languages.\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "  #For each language, the Multi30k is loaded with a specific 'split,' defined by the parameter. The 'language pair' parameter specifies the source and target language.\n",
        "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    #'build_vocab_from_iterator' builds the vocabulary from an iterator that yields lists of tokens. 'yield_tokens' iterates over('train_iter' function) and processes the datasets of the current language ('ln') and yeild the tokens used to build the vocabulary.\n",
        "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
        "                                                    min_freq=1, #Parameter that specifies the minimum frequency that a token must have to be included in the vocabulary is 1\n",
        "                                                    specials=special_symbols, #The list of special symbols added to the vocabulary\n",
        "                                                    special_first=True) #This parameter ensures that the special symbols are added to the beginning of the vocabulary\n",
        "# Sets the default index for tokens not found in the vocabulary to UNK_IDX.\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "  vocab_transform[ln].set_default_index(UNK_IDX)\n"
      ],
      "metadata": {
        "id": "2eoM8Pam3Q6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Model with Adaptive Layer\n",
        "\n",
        "We will now create a transformer that consists of an embedding layer, a trasformer model, and a linear layer. The embedding layer is responsible for converting the tensors of input indicies into corresponding tensors of input embeddings, aka embedding the input sequences into the latent space by converting token indicies into vectors of a specific size. The transformer processes the input data through a series of self-attention and feedforward neural network layers in the encoder layers, with each layer transforming the token into a more abstract representation until reaching the point of vectors. The linear layer then comes in to map the high-dimensional token representations into a new space whose dimensionality is equal to the size of the target vocabulary. The target vocabulary is the set of all possible output tokens of the transformer, which includes words, punctuation marks, etc."
      ],
      "metadata": {
        "id": "BXxPJDI2LN0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We will first import the necessary libraries from PyTorch for tensor operations.\n",
        "from torch import Tensor #\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import Transformer\n",
        "import math\n",
        "\n",
        "# Set the device to GPU(Graphics Processing Unit) if available; otherwise, use CPU(Central Processing Unit).\n",
        "#This allows leveraging hardware acceleration for training and inference, as GPU is generally acknowledged to be faster than the standard CPU especially in parallelizable tasks.\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Define a module for adding positional encodings to token embeddings.\n",
        "# Positional encodings provide the model with information about the position of tokens in the sequence by adding a position identifier to the vector representation of a token.\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, emb_size: int, dropout: float, maxlen: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        # Calculate positional encodings once in log space for efficiency.\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        # Add positional encodings to token embeddings and apply dropout.\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
        "\n",
        "# A module to convert token indices to embeddings. It maps tokens to vectors in a high-dimensional space.\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb_size: int):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        # Multiply embeddings by sqrt(emb_size) to normalize their scale.\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "# Define adaptive linear layers, which allow adjusting the model's capacity without retraining from scratch.\n",
        "class AdaptiveLinear(nn.Module):\n",
        "    def __init__(self, size: int):\n",
        "        super(AdaptiveLinear, self).__init__()\n",
        "        self.linear = nn.Linear(size, size)\n",
        "\n",
        "    def forward(self, x: Tensor):\n",
        "        # A simple linear transformation.\n",
        "        return self.linear(x)\n",
        "\n",
        "# Define adapter layers that can be inserted between transformer layers to fine-tune the model for specific tasks.\n",
        "class Adapter(nn.Module):\n",
        "    def __init__(self, size: int):\n",
        "        super(Adapter, self).__init__()\n",
        "        self.adapter_block = nn.Sequential(\n",
        "            nn.Linear(size, size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(size // 2, size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: Tensor):\n",
        "        # Apply the adapter block and add a skip connection.\n",
        "        ff_out = self.adapter_block(x)\n",
        "        adapter_out = ff_out + x\n",
        "        return adapter_out\n",
        "\n",
        "# The main Seq2Seq Transformer model combining the above components.\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self, num_encoder_layers: int, num_decoder_layers: int, emb_size: int,\n",
        "                 nhead: int, src_vocab_size: int, tgt_vocab_size: int,\n",
        "                 dim_feedforward: int = 512, dropout: float = 0.1, use_adaptive_layers: bool = False):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        self.transformer = Transformer(d_model=emb_size, nhead=nhead,\n",
        "                                       num_encoder_layers=num_encoder_layers,\n",
        "                                       num_decoder_layers=num_decoder_layers,\n",
        "                                       dim_feedforward=dim_feedforward, dropout=dropout)\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
        "        self.use_adaptive_layers = use_adaptive_layers\n",
        "        if use_adaptive_layers:\n",
        "            # Initialize adaptive layers if enabled.\n",
        "            self.adaptive_encoders = nn.ModuleList([Adapter(emb_size) for _ in range(num_encoder_layers - 1)])\n",
        "            self.adaptive_decoders = nn.ModuleList([Adapter(emb_size) for _ in range(num_decoder_layers - 1)])\n",
        "\n",
        "    def forward(self, src: Tensor, trg: Tensor, src_mask: Tensor, tgt_mask: Tensor,\n",
        "                src_padding_mask: Tensor, tgt_padding_mask: Tensor, memory_key_padding_mask: Tensor):\n",
        "        # Process input through the encoder, optional adaptive layers, and decoder to produce output.\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "        # Encoder\n",
        "        memory = src_emb\n",
        "        for i in range(len(self.transformer.encoder.layers)):\n",
        "            memory = self.transformer.encoder.layers[i](memory, src_mask)\n",
        "            if self.use_adaptive_layers and i < len(self.adaptive_encoders):\n",
        "                memory = self.adaptive_encoders[i](memory)\n",
        "        # Decoder\n",
        "        output = tgt_emb\n",
        "        for i in range(len(self.transformer.decoder.layers)):\n",
        "            output = self.transformer.decoder.layers[i](output, memory, tgt_mask)\n",
        "            if self.use_adaptive_layers and i < len(self.adaptive_decoders):\n",
        "                output = self.adaptive_decoders[i](output)\n",
        "        return self.generator(output)\n",
        "\n",
        "    # Additional methods for encoding, decoding, and toggling adaptive layers.\n",
        "\n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        return self.transformer.encoder(self.positional_encoding(\n",
        "                            self.src_tok_emb(src)), src_mask)\n",
        "\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        return self.transformer.decoder(self.positional_encoding(\n",
        "                          self.tgt_tok_emb(tgt)), memory,\n",
        "                          tgt_mask)\n",
        "\n",
        "    def toggle_adaptive_layers(self, use: bool):\n",
        "        \"\"\"\n",
        "        Toggle the usage of adaptive layers in the model.\n",
        "        :param use: A boolean flag to enable or disable adaptive layers.\n",
        "        \"\"\"\n",
        "        self.use_adaptive_layers = use"
      ],
      "metadata": {
        "id": "wk_SDHfpMJg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Masking Function\n",
        "\n",
        "We will now create a masking function. We have a masking function to hide the future tokens and the padding tokens during training, ensuring that the model predicts each token only based on the previous ones and ignoring the irrelevant information(in this case, the padding tokens).\n",
        "\n",
        "Paddings are special tokens added to the transformer sequences to make them all the same length. This is because the transformer requires inputs to be in uniform size. The padding needs to be hidden because they are irrelevant to the training.\n",
        "\n",
        "A collation function for preparing batches of data by padding sequences to a uniform length. -> aka, in this process, adding the special padding tokens.\n"
      ],
      "metadata": {
        "id": "mR1717xaO5tt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This function generates a square mask for the sequence. The mask ensures that during training,\n",
        "# the predictions for position i can depend only on the known outputs at positions less than i.\n",
        "def generate_square_subsequent_mask(sz):\n",
        "    # Creates an upper triangular matrix of ones, with zeros elsewhere. This matrix is then transposed.\n",
        "    # This operation ensures that for any position i in the sequence, positions > i are masked with `-inf`,\n",
        "    # making the model unable to peek into future tokens.\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
        "    # Converts the mask to float and changes 0s to `-inf` and 1s to 0.0.\n",
        "    # `-inf` is used to mask out the future tokens by setting their attention weight to 0,\n",
        "    # ensuring they don't contribute to the prediction of current and past tokens.\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "# This function creates four masks used during the forward pass of the transformer model.\n",
        "def create_mask(src, tgt):\n",
        "    # Calculates the sequence lengths of source and target tensors.\n",
        "    src_seq_len = src.shape[0]\n",
        "    tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "    # Generates a subsequent mask for target sequence to prevent the model from accessing future tokens.\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "    # Creates a mask with zeros for the source sequence, as self-attention for the source doesn't need masking.\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
        "\n",
        "    # Generates padding masks for source and target sequences. These masks ensure that model's attention mechanism\n",
        "    # ignores padding tokens by setting their positions to `True`. `PAD_IDX` is used to identify padding tokens.\n",
        "    # `.transpose(0, 1)` is applied to align the padding masks with the input tensor's shape for correct broadcasting.\n",
        "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
        "\n",
        "    # Returns the source mask, target mask, source padding mask, and target padding mask.\n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n"
      ],
      "metadata": {
        "id": "-WkIYYYdPNIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Weights and Parameters\n",
        "\n",
        "We will now set up the transformer model with Seq2Seq learning tasks - such as language translation - by defining its hyperparamters, initializing its weights, and preparing it for training."
      ],
      "metadata": {
        "id": "UDzR9imIPSyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set a fixed seed for reproducibility of results across different runs.\n",
        "torch.manual_seed(0)\n",
        "\n",
        "def create_German2English(vocab_transform, SRC_LANGUAGE = 'de', TGT_LANGUAGE = 'en'):\n",
        "    # Determine the sizes of the source and target vocabularies.\n",
        "    SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
        "    TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
        "\n",
        "    # Define the embedding size for input tokens. This size is also used in different parts of the transformer model.\n",
        "    EMB_SIZE = 512\n",
        "    # Define the number of heads in the multi-head attention mechanism. More heads allow the model to jointly attend to\n",
        "    # information at different positions from different representational spaces.\n",
        "    NHEAD = 8\n",
        "    # The dimension of the feedforward network model in nn.TransformerEncoder and nn.TransformerDecoder\n",
        "    FFN_HID_DIM = 512\n",
        "    # Batch size for training; affects the number of samples processed before the model is updated.\n",
        "    BATCH_SIZE = 128\n",
        "    # The number of layers in the transformer's encoder and decoder stacks.\n",
        "    NUM_ENCODER_LAYERS = 3\n",
        "    NUM_DECODER_LAYERS = 3\n",
        "\n",
        "    # Instantiate the Seq2SeqTransformer model with the specified parameters.\n",
        "    transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                    NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM, use_adaptive_layers=False)\n",
        "    return transformer\n",
        "\n",
        "def create_English2German(vocab_transform, SRC_LANGUAGE = 'en', TGT_LANGUAGE = 'de'):\n",
        "    SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
        "    TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
        "    # Define the embedding size for input tokens. This size is also used in different parts of the transformer model.\n",
        "    EMB_SIZE = 512\n",
        "    # Define the number of heads in the multi-head attention mechanism. More heads allow the model to jointly attend to\n",
        "    # information at different positions from different representational spaces.\n",
        "    NHEAD = 8\n",
        "    # The dimension of the feedforward network model in nn.TransformerEncoder and nn.TransformerDecoder\n",
        "    FFN_HID_DIM = 512\n",
        "    # Batch size for training; affects the number of samples processed before the model is updated.\n",
        "    BATCH_SIZE = 128\n",
        "    # The number of layers in the transformer's encoder and decoder stacks.\n",
        "    NUM_ENCODER_LAYERS = 3\n",
        "    NUM_DECODER_LAYERS = 3\n",
        "    # Instantiate the Seq2SeqTransformer model with the specified parameters.\n",
        "    transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                    NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM, use_adaptive_layers=False)\n",
        "    return transformer"
      ],
      "metadata": {
        "id": "qxt1RquQPpfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Collation Function\n",
        "\n",
        "We will now set up the collation function, which will convert a batch of raw strings with varying sizes into a batch of tensors of uniform sizes that can be directly fed into our model."
      ],
      "metadata": {
        "id": "NG9jluXHP3AZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the pad_sequence utility to pad sequences to the same length.\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# Defines a function that applies a series of transformations to the input text.\n",
        "# This is useful for chaining operations like tokenization, numericalization, and adding special tokens.\n",
        "def sequential_transforms(*transforms):\n",
        "    def func(txt_input):\n",
        "        for transform in transforms:\n",
        "            txt_input = transform(txt_input)  # Apply each transformation in sequence.\n",
        "        return txt_input\n",
        "    return func\n",
        "\n",
        "# Function to add Beginning Of Sentence (BOS) and End Of Sentence (EOS) tokens around the tokenized input,\n",
        "# and convert the sequence of token IDs into a PyTorch tensor.\n",
        "def tensor_transform(token_ids: List[int]):\n",
        "    return torch.cat((torch.tensor([BOS_IDX]),  # Prepend BOS token.\n",
        "                      torch.tensor(token_ids),  # Include the token IDs.\n",
        "                      torch.tensor([EOS_IDX])))  # Append EOS token.\n",
        "\n",
        "# Dictionary to hold the transformations for both source and target languages.\n",
        "text_transform = {}\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    text_transform[ln] = sequential_transforms(token_transform[ln],  # Apply tokenization.\n",
        "                                               vocab_transform[ln],  # Convert tokens to numerical IDs.\n",
        "                                               tensor_transform)  # Add BOS/EOS tokens and convert to tensor.\n",
        "\n",
        "# Function to collate a batch of data points. This function is used by the DataLoader to combine individual\n",
        "# data items into a batch.\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []  # Lists to hold source and target sequences for the batch.\n",
        "    for src_sample, tgt_sample in batch:  # Iterate over each data point in the batch.\n",
        "        # Process the source and target samples, strip trailing newline characters, and apply text transformations.\n",
        "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
        "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
        "\n",
        "    # Pad the sequences in the batch to the same length and convert to tensors.\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
        "    return src_batch, tgt_batch  # Return the processed batch.\n"
      ],
      "metadata": {
        "id": "MWOVxWtyQJnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining the Training Loop\n",
        "\n",
        "We will now define the training and evaluation loops for our Seq2Seq transformer model. The training loop is responsible for iterating over the training data, making predictions, calculating loss(based on the loss function), and updating the model parameters(with the optimizer to minimize the loss). This is the training loop. There are a few key compoennet to do anytime\n",
        "you are triainign a model. The very first thing you do is zero the gradient. This\n",
        " means that you are starting a fresh slate for every batch of data. This way,\n",
        " you don't combine multiple batches of data (you don't want to be carrying\n",
        " over the gradient) -> Line 29. (You have to zero the gradient before calculating\n",
        "  the loss. Usually, Line 29 should go before you calculat the loss and logits. Line 27. )\n",
        "Then, you need to make sure that you are making a prediction.\n",
        "Then, you calculate the loss.  Then you do (line 34) loss.backward, which is\n",
        "how you update the model weights - or how the ML learns.\n",
        "Line(38) - you just want to take a step with the optimizer - minimize the loss.\n",
        "Then, you calculate some metrics."
      ],
      "metadata": {
        "id": "oc4iX4RvQar3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.datasets import Multi30k\n",
        "import torchtext\n",
        "from torch.utils.data import IterableDataset\n",
        "\n",
        "# Import necessary modules for data handling\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "#make it accept the train_iter\n",
        "\n",
        "def train_epoch(model, train_iter, optimizer, epoch, SRC_LANGUAGE, TGT_LANGUAGE, best_loss=float('inf')):\n",
        "  #Initializing the best_loss as infinity so that anything below that is now considered the best.\n",
        "    model.train()  # Set the model to training mode\n",
        "    losses = 0\n",
        "    BATCH_SIZE = 128\n",
        "\n",
        "    # Assuming train_dataloader is defined and set up elsewhere\n",
        "    print(torchtext.__version__) # 0.16\n",
        "    train_iter = Multi30k(split='train', language_pair=('en', 'de'))\n",
        "    print (type(train_iter))\n",
        "    #TODO how do we combine the other dataset with this 'train_iter'.\n",
        "    #See if train_iter is a list or set or anything else.\n",
        "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "    n_batches = 0\n",
        "    for src, tgt in train_dataloader:\n",
        "\n",
        "        src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n",
        "        tgt_input = tgt[:-1, :]\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses += loss.item()\n",
        "        n_batches +=1\n",
        "\n",
        "    avg_loss = losses / n_batches\n",
        "\n",
        "    # Check if the average loss of this epoch is the best so far\n",
        "    if avg_loss < best_loss:\n",
        "        best_loss = avg_loss  # Update the best loss with the current average loss\n",
        "        # Save the model and any other components\n",
        "        if (SRC_LANGUAGE == 'de'):\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                #It is saving that anything that needs any kind of weight associated with it. Now there is a weight to load it in\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                #We are saving the optimizer for the training - not for the inference: if you want to use the train model for a translation.\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': best_loss,\n",
        "          }, '/content/drive/MyDrive/Etruscan_Project/model_best_forward.pth')  # Save to a .pth file\n",
        "          #MAKE THIS /slfwdof SO THAT IT IS SAVED TO THE GOOGLE DRIVE\n",
        "        else:\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "               #It is saving that anything that needs any kind of weight associated with it. Now there is a weight to load it in\n",
        "                'model_state_dict': model.state_dict(),\n",
        "               #We are saving the optimizer for the training - not for the inference: if you want to use the train model for a translation.\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': best_loss,\n",
        "          }, '/content/drive/MyDrive/Etruscan_Project/model_best_backward.pth')  # Save to a .pth file\n",
        "\n",
        "    print(f'Epoch {epoch}, Loss: {avg_loss}, Best Loss: {best_loss}')\n",
        "\n",
        "    return avg_loss, best_loss  # Return the average loss and the best loss for tracking\n",
        "\n",
        "class CombinedDataset(IterableDataset):\n",
        "    def __init__(self, data_pipe1, data_pipe2):\n",
        "        self.data_pipe1 = data_pipe1\n",
        "        self.data_pipe2 = data_pipe2\n",
        "    def __iter__(self):\n",
        "        for item in self.data_pipe1:\n",
        "            yield item\n",
        "        for item in self.data_pipe2:\n",
        "            yield item\n",
        "\n",
        "# Function to evaluate the model's performance on the validation set\n",
        "\n",
        "def evaluate(model):\n",
        "    # Set the model to evaluation mode to disable dropout, batch normalization, etc.\n",
        "    model.eval()\n",
        "\n",
        "    # Initialize a variable to accumulate the total loss\n",
        "    losses = 0\n",
        "\n",
        "    BATCH_SIZE = 128\n",
        "\n",
        "    # Load the validation dataset\n",
        "    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    # Prepare data loader for validation data\n",
        "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "    # Iterate over each batch of validation data\n",
        "    for src, tgt in val_dataloader:\n",
        "        # Move source and target tensors to the appropriate device (GPU/CPU)\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        # Prepare the input for the target by removing the last token\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        # Create masks and padding masks for source and target inputs\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        # Forward pass: compute the predicted outputs (logits) from the model without updating model parameters\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        # Prepare the target outputs by removing the first token (BOS)\n",
        "        tgt_out = tgt[1:, :]\n",
        "        # Compute the loss between the predicted logits and the actual target outputs\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        # Accumulate the loss\n",
        "        losses += loss.item()\n",
        "\n",
        "    # Return the average loss over the validation data\n",
        "    return losses / len(list(val_dataloader))\n"
      ],
      "metadata": {
        "id": "80hfTmUqRFuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Model\n",
        "\n",
        "Now, we have all the hyperparameters, functions, and dictionaries to train our model! Lets define our Epoch, utilize the Greedy function to take the best at every timestamp, and use the translating function to complete the translation!"
      ],
      "metadata": {
        "id": "hEoLJenURKgx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchtext.datasets import Multi30k\n",
        "from torch.utils.data.datapipes.iter import IterableWrapper\n",
        "from itertools import chain\n",
        "from timeit import default_timer as timer\n",
        "import numpy as np\n",
        "\n",
        "#Here, they are just calling the training loop functions.\n",
        "#An EPOCH is one full iteration through your training data\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "#This is the general trainer\n",
        "def train_model(NUM_EPOCH, transformer, optimizer, SRC_LANGUAGE, TGT_LANGUAGE, patience=2):\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(1, NUM_EPOCHS + 1):\n",
        "        start_time = timer()\n",
        "        # Assume train_epoch function returns average and best loss for the epoch\n",
        "        train_avg_loss, train_best_loss = train_epoch(transformer, train_iter, optimizer, epoch, SRC_LANGUAGE, TGT_LANGUAGE, best_val_loss)\n",
        "        end_time = timer()\n",
        "        val_loss = evaluate(transformer)\n",
        "\n",
        "        print(f\"Epoch: {epoch}, Train Avg loss: {train_avg_loss:.3f}, Train Best loss: {train_best_loss: .3f}, \"\n",
        "              f\"Val loss: {val_loss:.3f}, Epoch time = {(end_time - start_time):.3f}s\")\n",
        "\n",
        "        # Early stopping logic\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0  # Reset patience counter\n",
        "        else:\n",
        "            patience_counter += 1  # Increment patience counter\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "NUM_EPOCHS = 100\n",
        "\n",
        "transformer_forward = create_German2English(vocab_transform)\n",
        "\n",
        "transformer_backward = create_English2German(vocab_transform)\n",
        "\n",
        "for transformer in (transformer_forward, transformer_backward):\n",
        "  # Initialize the model's parameters using the Xavier uniform initialization method.\n",
        "  # This initialization helps in keeping the signal from exploding or vanishing in deep networks.\n",
        "  for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "  # Move the model to the appropriate device (GPU or CPU) for computation efficiency.\n",
        "  transformer = transformer.to(DEVICE)\n",
        "\n",
        "#Here, we are trying to combine the datasets\n",
        "\n",
        "# Load the Multi30k dataset\n",
        "multi30k_train_iter = Multi30k(split='train', language_pair=('en', 'de'))\n",
        "\n",
        "\n",
        "# In a nutshell - neural networks work by minimizing the loss. In order to\n",
        "#minimize the loss, you folloow a stochastic gradient descent - you take in a batch of data,\n",
        "# you evaluate the performance on the patch of data and calculate the loss, then\n",
        "#, based on the loss, your network will update its weights ideally to increase\n",
        "#performance and minimize the loss further - the process to do this is accomplished\n",
        "#by an optimizer. There are a lot of optimizers, Adam optimizers is one of them..\n",
        "\n",
        "# In a nutshell - neural networks work by minimizing the loss. In order to\n",
        "#minimize the loss, you folloow a stochastic gradient descent - you take in a batch of data,\n",
        "# you evaluate the performance on the patch of data and calculate the loss, then\n",
        "#, based on the loss, your network will update its weights ideally to increase\n",
        "#performance and minimize the loss further - the process to do this is accomplished\n",
        "#by an optimizer. There are a lot of optimizers, Adam optimizers is one of them..\n",
        "\n",
        "# Define the loss function. CrossEntropyLoss is used for classification tasks.\n",
        "# The `ignore_index` parameter is set to PAD_IDX to exclude the padding tokens from the loss calculation.\n",
        "\n",
        "for index, transformer in enumerate([transformer_forward, transformer_backward]):\n",
        "    if (index == 0):\n",
        "        SRC_LANGUAGE = 'de'\n",
        "        TGT_LANGUAGE = 'en'\n",
        "    else:\n",
        "        SRC_LANGUAGE = 'en'\n",
        "        TGT_LANGUAGE = 'de'\n",
        "\n",
        "    optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "    train_model(NUM_EPOCHS, transformer, optimizer, SRC_LANGUAGE, TGT_LANGUAGE)\n",
        "\n",
        "#This is where the back translation needs to happen. We would basically be creating a new backtranslated dataset and it needs to be integrated into the training loop.\n",
        "# function to generate output sequence using greedy algorithm\n",
        "#Algorithm that takes the best at every timestep - som\n",
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    src = src.to(DEVICE)\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(DEVICE)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
        "                    .type(torch.bool)).to(DEVICE)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "    return ys\n",
        "\n",
        "\n",
        "# actual function to translate input sentence into target language\n",
        "def translate(model: torch.nn.Module, src_sentence: str):\n",
        "    model.eval()\n",
        "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = greedy_decode(\n",
        "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
        "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
      ],
      "metadata": {
        "id": "WL6a8s-LRk1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Back Translation\n",
        "\n",
        "This is the back trnalsation."
      ],
      "metadata": {
        "id": "wGPQHPnEM93f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext"
      ],
      "metadata": {
        "id": "cSlaytArxGnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, IterableDataset\n",
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "# Ensure you have the necessary imports for your translate functions and any other utilities you use\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "SRC_LANGUAGE = 'en'\n",
        "TGT_LANGUAGE = 'de'\n",
        "# Define BOS_IDX, EOS_IDX, and PAD_IDX based on your vocabulary\n",
        "\n",
        "\n",
        "# Back_tranlsated = [\n",
        "# Def back_translate (data_loader, transformer_forward, transformer_backward):\n",
        "# \tFor src, tgt in data_loader:\n",
        "# \t\tOutput = translate (transfoemrt_forward, src)\n",
        "# \t\tBack_translation = translate(transofmer_backward, output)\n",
        "# \t\tSample = (src, back_translation)\n",
        "# Back_translated.append(sample)\n",
        "\n",
        "# !pip install torchtext\n",
        "\n",
        "# import torchtext.data.transforms\n",
        "\n",
        "# from torchtext.data.transforms import TextTransform\n",
        "\n",
        "def back_translate (data_loader, transformer_forward, transformer_backward):\n",
        "  back_translated = []\n",
        "  for src, tgt in data_loader:\n",
        "    # src_sentence = text_transform[SRC_LANGUAGE].decode(src[0])\n",
        "    #src_sentence = src_field.decode(src[0])\n",
        "    output = translate(transformer_forward, src)\n",
        "    back_translation = translate(transformer_backward, output)\n",
        "    sample = (src, back_translation)\n",
        "    back_translated.append(sample)\n",
        "  return back_translated\n",
        "\n",
        "# text_transform = {\n",
        "#     SRC_LANGUAGE: TextTransform(),\n",
        "#     TGT_LANGUAGE: TextTransform(),\n",
        "# }"
      ],
      "metadata": {
        "id": "DLiYEDZZM-VV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Re-training"
      ],
      "metadata": {
        "id": "tANc-k2KNlJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchtext\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "train_iter = Multi30k(split='train', language_pair=('en', 'de'))\n",
        "train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "# Your custom backtranslated dataset (list of tuples)\n",
        "backtranslated_data = back_translate(train_dataloader, transformer_forward, transformer_backward)\n",
        "\n",
        "# Convert your custom dataset to a DataPipe\n",
        "backtranslated_iter = IterableWrapper(backtranslated_data)\n",
        "\n",
        "#Now, for the chain concatenation system.\n",
        "#It should be in a tuple - [(src, tgt), ...]\n",
        "\n",
        "train_dataloader = DataLoader(combined_iter, batch_size=4)\n",
        "\n",
        "from torch.utils.data import IterableDataset\n",
        "class CombinedDataset(IterableDataset):\n",
        "    def __init__(self, data_pipe1, data_pipe2):\n",
        "        self.data_pipe1 = data_pipe1\n",
        "        self.data_pipe2 = data_pipe2\n",
        "    def __iter__(self):\n",
        "        for item in self.data_pipe1:\n",
        "            yield item\n",
        "        for item in self.data_pipe2:\n",
        "            yield item\n",
        "\n",
        "#Usage\n",
        "combined_dataset = CombinedDataset(train_iter, backtranslated_iter)\n",
        "combine_dataloader = DataLoader(combined_dataset, BATCH_SIZE)"
      ],
      "metadata": {
        "id": "JJs7wXw6xGET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_backtranslation = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "train_model(NUM_EPOCH=1, transformer_forward, optimizer_backtranslation, SRC_LANGUAGE, TGT_LANGUAGE)\n",
        "\n"
      ],
      "metadata": {
        "id": "bybclF2B2M3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaulation with BLEU score\n",
        "\n",
        "We will now evaluate the model with the BLEU score."
      ],
      "metadata": {
        "id": "nV9K4UcKGxmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    src = src.to(DEVICE)\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(DEVICE)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
        "                    .type(torch.bool)).to(DEVICE)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "    return ys\n",
        "\n",
        "# actual function to translate input sentence into target language\n",
        "def translate(model: torch.nn.Module, src_sentence: str):\n",
        "    model.eval()\n",
        "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = greedy_decode(\n",
        "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
        "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
      ],
      "metadata": {
        "id": "PomI7OYK0QHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Load the saved file\n",
        "checkpoint_forward = torch.load('/content/drive/MyDrive/Etruscan_Project/model_best_forward.pth')\n",
        "checkpoint_backward = torch.load('/content/drive/MyDrive/Etruscan_Project/model_best_backward.pth')\n",
        "\n",
        "transformer_forward = create_German2English(vocab_transform)\n",
        "transformer_backward = create_English2German(vocab_transform)\n",
        "\n",
        "# Load the model state dictionary\n",
        "transformer_forward.load_state_dict(checkpoint_forward['model_state_dict'])\n",
        "transformer_backward.load_state_dict(checkpoint_backward['model_state_dict'])\n",
        "\n",
        "transformer_forward.to(DEVICE)\n",
        "transformer_backward.to(DEVICE)\n",
        "\n",
        "print(translate(transformer_backward, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))\n",
        "\n",
        "print(translate(transformer_forward, \"The boy runs into the house\"))\n",
        "\n",
        "# predictions = []\n",
        "# references = []\n",
        "# results = bleu.compute(predictions=predictions, references=references)\n",
        "# print(results)"
      ],
      "metadata": {
        "id": "gJcgooNbPz3w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}